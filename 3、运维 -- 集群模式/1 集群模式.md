
尽管Redis哨兵模式


# 

Redis集群不支持多个数据库。Redis集群只支持0号数据库，同时SELECT命令被禁用。

# Cluster Bus（集群总线）

Redis集群每个节点除了数据端口，还有一个用来节点互相通信的端口。这个端口如果没有在配置文件中指定，那么就是数据端口假10000。

Redis集群中的每个节点都通过集群总线互通，使用gossip协议来交换信息，1）以发现其他新节点，2）发送ping包以确定其他节点正常工作，3）发送特定条件下的集群信息。集群总线还用于在整个集群中传播 Pub/Sub 消息，并在用户请求时协调手动故障转移(手动故障转移不是由 Redis 集群故障检测器启动的，而是由系统管理员直接启动的)。

由于集群节点不能代理请求，客户端可能会使用重定向错误(MOVED 和 -ASK)被重定向到其他节点。理论上，客户机可以自由地向集群中的所有节点发送请求，如果需要，还可以进行重定向，因此不需要客户机保存集群的状态。然而，能够在键和节点之间缓存映射的客户机可以明显地提高性能。

# 写安全性

Redis集群节点间仍然使用异步复制，即客户端向集群其中一个节点写入后，该节点直接返回写确认，而不会等待写操作传播到其他节点。

在此之前，先介绍一个官网里提到的单词 partitions。字面意思分区，集群可能因各种问题（如网络问题）被分成多个分区，主机数量多的区称为majority，个人记作M，主机数量少的区称为minority，个人记作m。集群正常工作时是一个区。

在分区期间，是有一个时间窗口期间可能出现写丢失的，这个窗口在客户端连接到M和m是存在不同的。

下面是导致失败期间在M中收到的已确认的写操作发生丢失的情况的例子：

M中的主机收到了来自客户端的写操作，并向客户端发送了写确认，但是写操作却没能异步复制到从机。如果主机在没有到达副本的情况下死亡，那么如果主服务器在足够长的时间内无法到达，以至于其中一个副本被提升，那么该写操作将永远丢失。在主节点完全突然死亡的情况下，通常很难观察到这一点，因为主节点几乎同时尝试响应客户端(确认写入)和副本(传播写入)。然而，这是一个现实世界的失败模式。

另一种理论上可能出现的写丢失的故障模式如下。
一台主机（记为A）因为分区变得不可达，随后A被它的一台从机（记为B）故障迁移，过了一段时间，A又变得可达，在A转变成B的从机之前，客户端仍能向A中写入数据。

The second failure mode is unlikely to happen because master nodes unable to communicate with the majority of the other masters for enough time to be failed over will no longer accept writes, and when the partition is fixed writes are still refused for a small amount of time to allow other nodes to inform about configuration changes. This failure mode also requires that the client's routing table has not yet been updated.

Writes targeting the minority side of a partition have a larger window in which to get lost. For example, Redis Cluster loses a non-trivial number of writes on partitions where there is a minority of masters and at least one or more clients, since all the writes sent to the masters may potentially get lost if the masters are failed over in the majority side.

Specifically, for a master to be failed over it must be unreachable by the majority of masters for at least `NODE_TIMEOUT`, so if the partition is fixed before that time, no writes are lost. When the partition lasts for more than `NODE_TIMEOUT`, all the writes performed in the minority side up to that point may be lost. However the minority side of a Redis Cluster will start refusing writes as soon as `NODE_TIMEOUT` time has elapsed without contact with the majority, so there is a maximum window after which the minority becomes no longer available. Hence, no writes are accepted or lost after that time.

# 可用性

在分区的大多数端，假设至少有大多数主机和每个不可到达的主机的副本，集群在 NODE _ TIMEOUT 时间之后再次可用，再加上一个副本选择和故障转移它的主服务器所需的几秒钟(故障转移通常在1或2秒内执行)。

这意味着 Redis 集群的设计能够承受集群中少数节点的故障，但是对于在发生大规模网络分裂时需要可用性的应用程序来说，这不是一个合适的解决方案。

在一个由 N 个主节点组成的集群的例子中，其中每个节点都有一个副本，只要一个节点被分区，集群的大多数侧将保持可用，并且当两个节点被分区时，集群的大多数侧将以1-(1/(N * 2-1))的概率保持可用(在第一个节点失败之后，我们剩下 N * 2-1个节点，唯一没有副本失败的主节点的概率是1/(N * 2-1))。

例如，在一个有5个节点和每个节点一个副本的集群中，有一个1/(5 * 2-1) = 11.11% 的概率，在两个节点被从多数节点分区之后，集群将不再可用。

由于 Redis 集群的一个称为副本迁移的特性，在许多实际场景中，由于副本迁移到孤立的主服务器(主服务器不再拥有副本) ，集群的可用性得到了改善。因此，在每次成功的失败事件中，集群可能会重新配置副本布局，以便更好地抵御下一次失败。

# Performance

In Redis Cluster nodes don't proxy commands to the right node in charge for a given key, but instead they redirect clients to the right nodes serving a given portion of the key space.

Eventually clients obtain an up-to-date representation of the cluster and which node serves which subset of keys, so during normal operations clients directly contact the right nodes in order to send a given command.

Because of the use of asynchronous replication, nodes do not wait for other nodes' acknowledgment of writes (if not explicitly requested using the [`WAIT`](https://redis.io/commands/wait) command).

Also, because multi-key commands are only limited to _near_ keys, data is never moved between nodes except when resharding.

Normal operations are handled exactly as in the case of a single Redis instance. This means that in a Redis Cluster with N master nodes you can expect the same performance as a single Redis instance multiplied by N as the design scales linearly. At the same time the query is usually performed in a single round trip, since clients usually retain persistent connections with the nodes, so latency figures are also the same as the single standalone Redis node case.

Very high performance and scalability while preserving weak but reasonable forms of data safety and availability is the main goal of Redis Cluster.

# Why merge operations are avoided

The Redis Cluster design avoids conflicting versions of the same key-value pair in multiple nodes as in the case of the Redis data model this is not always desirable. Values in Redis are often very large; it is common to see lists or sorted sets with millions of elements. Also data types are semantically complex. Transferring and merging these kind of values can be a major bottleneck and/or may require the non-trivial involvement of application-side logic, additional memory to store meta-data, and so forth.

There are no strict technological limits here. CRDTs or synchronously replicated state machines can model complex data types similar to Redis. However, the actual run time behavior of such systems would not be similar to Redis Cluster. Redis Cluster was designed in order to cover the exact use cases of the non-clustered Redis version.


