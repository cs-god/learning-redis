
下面包含Redis集群相关的所有配置项。（注释以英译为主）
```text
################################ REDIS CLUSTER  ###############################

# 是否作为集群节点启动 
#   - 普通的Redis实例不能作为Redis集群的节点,只有以作为集群节点的方式启动的Redis实例才可以
cluster-enabled yes

# 集群节点配置文件
#   - 每个集群节点有自己的集群配置文件
#   - 该文件不是通过手动编辑
#   - 该文件被Redis节点创建和更新
#   - 运行在相同系统中的集群节点需要有不同的集群配置文件,以防止互相覆盖
cluster-config-file nodes-6379.conf

# 集群节点端口: 
#   - 集群bus监听入内连接的端口
#   - 当设置成默认值0时,端口值=监听命令行请求端口+10000
cluster-port 0


```
# failover相关配置项

如果一个主机的从机的数据太"老"的话，从机是不会尝试进行failover的。

但是对于从机而言,没有一个简单的方式知道它数据的"年龄",因此进行下面2个检查：
1）如果有多个从机能够能够进行failover，那么它们会互相交换offset信息,然后会根据offset排队
2）每个从机会计算它与它的主机最后交互的时间，这可能是：
- time向主机发送心跳包(PING)的时间点
- 从主机收到的命令的时间点(如果主机仍然处于"connected"状态)
- 与主机失去连接的时间点(如果从机的连接下线)

如果最后一次交互的时间点太"久远",那么从机不会参与failover,久远指的是:
 (node-timeout * cluster-replica-validity-factor) + repl-ping-replica-period

例子：
- node-timeout=30 sec
- cluster-replica-validity-factor=10
- repl-ping-replica-period=10 sec
那么，如果从机与主机实去连接超过310秒,该从机将不会再尝试failover。


可以直接禁止当主机fails时从机进行failover。这在我们希望某个集群节点永远不会被提升为主机时有用。

```text
# 
cluster-replica-no-failover no
```


```text
# 集群节点最大不可用时长
#   - 在这个时长内,集群节点不会被判定为fail。
#   - 对于master节点,当不可用时长超过此值时,它的slave在延迟至少0.5秒后会发起选举进行failover成为master
#   - Redis集群的很多其它值与cluster-node-timeout有关。
#   - 以ms为单位
cluster-node-timeout 15000

# A large cluster-replica-validity-factor may allow replicas with too old data to failover
# a master, while a too small value may prevent the cluster from being able to
# elect a replica at all.
#
# For maximum availability, it is possible to set the cluster-replica-validity-factor
# to a value of 0, which means, that replicas will always try to failover the
# master regardless of the last time they interacted with the master.
# (However they'll always try to apply a delay proportional to their
# offset rank).
# 零是唯一能够保证当所有分区恢复时集群将始终能够继续。
cluster-replica-validity-factor 10

# 心跳(PING)间隔时间
repl-ping-replica-period 0







```

# 从机迁移

从机迁移：从机能够迁移到"orphaned"主机（该主机下没有其它正在工作中的从机），这能够改善集群的可用性，因为如果没有从机迁移，那么"orphaned"主机宕机了，将会没有从机来顶替主机继续工作。

发生的条件：只会发生在它所属的主机目前在该从机迁移到"orphaned"主机后仍然至少有指定数量的从机正在工作时。

配置项：`cluster-migration-barrier`，会影响希望每个主机搭配的从机数量。

例子：
- `cluster-allow-replica-migration no` ：禁用从机迁移
- `cluster-migration-barrier 极大数`：等效于禁用从机迁移
- `cluster-migration-barrier 0`：只适用于debug，在生产环境中是危险的。

```text

# 是否开启从机迁移
# cluster-allow-replica-migration yes

# 从机迁移发生时主机必须有的最小从机数量
# cluster-migration-barrier 1

```

# 哈希槽全映射

默认情况下，如果Redis集群检测到目前集群中的所有主机没有覆盖掉所有的Hash Slot，那么Redis集群将会拒绝接受查询。

这样，如果发生集群缩容，那么集群将会变得不可用。因此`cluster-require-full-coverage`控制该行为。

```text
# 是否运行集群中所有主机没有覆盖全部的Hash Slot时仍然对外提供服务:
#   - yes: 不允许
#   - no:  允许

cluster-require-full-coverage yes

```


# 没有介绍完的配置项

```text
# This option, when set to yes, allows nodes to serve read traffic while the
# cluster is in a down state, as long as it believes it owns the slots.
#
# This is useful for two cases.  The first case is for when an application
# doesn't require consistency of data during node failures or network partitions.
# One example of this is a cache, where as long as the node has the data it
# should be able to serve it.
#
# The second use case is for configurations that don't meet the recommended
# three shards but want to enable cluster mode and scale later. A
# master outage in a 1 or 2 shard configuration causes a read/write outage to the
# entire cluster without this option set, with it set there is only a write outage.
# Without a quorum of masters, slot ownership will not change automatically.
#
# cluster-allow-reads-when-down no

# This option, when set to yes, allows nodes to serve pubsub shard traffic while
# the cluster is in a down state, as long as it believes it owns the slots.
#
# This is useful if the application would like to use the pubsub feature even when
# the cluster global stable state is not OK. If the application wants to make sure only
# one shard is serving a given channel, this feature should be kept as yes.
#
# cluster-allow-pubsubshard-when-down yes

# Cluster link send buffer limit is the limit on the memory usage of an individual
# cluster bus link's send buffer in bytes. Cluster links would be freed if they exceed
# this limit. This is to primarily prevent send buffers from growing unbounded on links
# toward slow peers (E.g. PubSub messages being piled up).
# This limit is disabled by default. Enable this limit when 'mem_cluster_links' INFO field
# and/or 'send-buffer-allocated' entries in the 'CLUSTER LINKS` command output continuously increase.
# Minimum limit of 1gb is recommended so that cluster link buffer can fit in at least a single
# PubSub message by default. (client-query-buffer-limit default value is 1gb)
#
# cluster-link-sendbuf-limit 0
# Clusters can configure their announced hostname using this config. This is a common use case for
# applications that need to use TLS Server Name Indication (SNI) or dealing with DNS based
# routing. By default this value is only shown as additional metadata in the CLUSTER SLOTS
# command, but can be changed using 'cluster-preferred-endpoint-type' config. This value is
# communicated along the clusterbus to all nodes, setting it to an empty string will remove
# the hostname and also propagate the removal.
#
# cluster-announce-hostname ""

# Clusters can advertise how clients should connect to them using either their IP address,
# a user defined hostname, or by declaring they have no endpoint. Which endpoint is
# shown as the preferred endpoint is set by using the cluster-preferred-endpoint-type
# config with values 'ip', 'hostname', or 'unknown-endpoint'. This value controls how
# the endpoint returned for MOVED/ASKING requests as well as the first field of CLUSTER SLOTS.
# If the preferred endpoint type is set to hostname, but no announced hostname is set, a '?'
# will be returned instead.
#
# When a cluster advertises itself as having an unknown endpoint, it's indicating that
# the server doesn't know how clients can reach the cluster. This can happen in certain
# networking situations where there are multiple possible routes to the node, and the
# server doesn't know which one the client took. In this case, the server is expecting
# the client to reach out on the same endpoint it used for making the last request, but use
# the port provided in the response.
#
# cluster-preferred-endpoint-type ip


```